# ЛИСТ ОШИБОК И РЕШЕНИЙ

**Проект:** Автоматизация анализа конкурентов (Python + Selenium)
**Дата создания:** 22.01.2026
**Назначение:** Документирование всех ошибок и их решений

---

## КАК ИСПОЛЬЗОВАТЬ ЭТОТ ФАЙЛ

1. При возникновении ошибки - сразу записать её сюда
2. Указать дату, контекст, текст ошибки
3. Описать попытки решения
4. Записать финальное решение
5. Добавить примечания для будущего

**Формат записи:**
```markdown
### ОШИБКА #N: Краткое описание

**Дата:** 22.01.2026
**Компонент:** yandex_parser.py
**Критичность:** Высокая/Средняя/Низкая

**Описание:**
Подробное описание проблемы

**Текст ошибки:**
\`\`\`
Вывод ошибки
\`\`\`

**Попытки решения:**
1. Попытка 1 - не сработало
2. Попытка 2 - не сработало

**Решение:**
Что помогло

**Примечания:**
Дополнительная информация
```

---

## ИЗВЕСТНЫЕ ПРОБЛЕМЫ И РЕШЕНИЯ

### КАТЕГОРИЯ 1: ЯНДЕКС АНТИБОТ ЗАЩИТА

#### ОШИБКА #001: Яндекс показывает пустую страницу вместо результатов поиска

**Дата:** Прогнозируемая ошибка
**Компонент:** yandex_parser.py
**Критичность:** Критическая

**Описание:**
При использовании обычного Selenium WebDriver, Яндекс определяет автоматизацию и показывает пустую страницу или перенаправляет на CAPTCHA.

**Симптомы:**
- Страница долго загружается
- Вместо результатов поиска пустая страница
- В консоли браузера ошибки JavaScript
- Селектор `.serp-item` не находит элементы

**Текст ошибки:**
```python
selenium.common.exceptions.NoSuchElementException: Message: no such element:
Unable to locate element: {"method":"css selector","selector":".serp-item"}
```

**Попытки решения:**
1. ❌ Использование обычного Selenium - не работает
2. ❌ Headless mode - еще хуже, Яндекс сразу блокирует
3. ❌ Изменение User-Agent - недостаточно

**Решение:**
✅ Использовать `undetected-chromedriver` + реальный Chrome профиль + selenium-stealth

```python
import undetected_chromedriver as uc
from selenium_stealth import stealth

options = uc.ChromeOptions()
# КЛЮЧЕВОЕ: использование реального профиля
options.add_argument("--user-data-dir=/Users/ivan/Library/Application Support/Google/Chrome")
options.add_argument("--profile-directory=Default")

driver = uc.Chrome(options=options, version_main=120)

stealth(driver,
    languages=["ru-RU", "ru"],
    vendor="Google Inc.",
    platform="MacIntel",
    webgl_vendor="Intel Inc.",
    renderer="Intel Iris OpenGL Engine",
    fix_hairline=True,
)
```

**Примечания:**
- НЕ использовать headless режим для Яндекса
- Обновлять профиль Chrome раз в неделю
- Добавлять задержки 2-5 сек между запросами

---

#### ОШИБКА #002: SmartCaptcha блокирует доступ

**Дата:** Прогнозируемая ошибка
**Компонент:** yandex_parser.py
**Критичность:** Высокая

**Описание:**
При частых запросах Яндекс показывает SmartCaptcha (капчу от Яндекса).

**Симптомы:**
- Вместо результатов поиска появляется задание с картинками
- URL содержит `/checkcaptcha` или `/showcaptcha`
- Невозможно получить результаты поиска

**Текст ошибки:**
```python
# Ошибка может не выводиться, но в driver.current_url будет:
# https://yandex.ru/checkcaptcha?...
```

**Попытки решения:**
1. ❌ Игнорирование - невозможно продолжить парсинг
2. ❌ Автоматическое решение через Selenium - сложно

**Решение:**
✅ Профилактические меры:

1. Использовать сохраненные cookies после ручной авторизации
```python
import pickle

def load_cookies(driver, filepath="cookies/yandex.pkl"):
    driver.get("https://yandex.ru")
    with open(filepath, 'rb') as f:
        cookies = pickle.load(f)
        for cookie in cookies:
            if 'expiry' in cookie:
                del cookie['expiry']
            driver.add_cookie(cookie)
    driver.refresh()
```

2. Увеличить задержки между запросами
```python
import random
import time

time.sleep(random.uniform(5, 10))  # 5-10 сек между запросами
```

3. Ограничить количество запросов (не более 10-15 в час)

**Альтернативное решение (если профилактика не помогла):**
Интеграция с 2captcha API (платно: ~$3 за 1000 капч)

```python
from twocaptcha import TwoCaptcha

solver = TwoCaptcha('YOUR_API_KEY')
result = solver.yandex(sitekey='...', url='...')
# Отправка результата капчи
```

**Примечания:**
- Сохранять cookies каждый раз после успешного парсинга
- Обновлять cookies раз в 5-7 дней
- Не делать больше 10 запросов подряд

---

#### ОШИБКА #003: Cookies устарели или невалидны

**Дата:** Прогнозируемая ошибка
**Компонент:** utils/browser.py
**Критичность:** Средняя

**Описание:**
Сохраненные cookies Яндекса устарели, и Яндекс требует повторной авторизации.

**Симптомы:**
- Яндекс показывает форму входа
- Просит подтвердить номер телефона
- Результаты поиска не отображаются

**Текст ошибки:**
```python
# Ошибки может не быть, но driver.current_url будет:
# https://passport.yandex.ru/auth/...
```

**Решение:**
✅ Регулярно обновлять cookies

1. Вручную авторизоваться в Chrome профиле
```bash
# Открыть Chrome с нужным профилем
/Applications/Google\ Chrome.app/Contents/MacOS/Google\ Chrome \
  --user-data-dir="/Users/ivan/Library/Application Support/Google/Chrome" \
  --profile-directory=Default
```

2. Зайти на yandex.ru, авторизоваться
3. Сделать 2-3 реальных поиска
4. Закрыть браузер
5. Запустить скрипт сохранения cookies

```python
from utils.browser import init_driver, save_cookies

driver = init_driver()
driver.get("https://yandex.ru")
input("Авторизуйся вручную, потом нажми Enter...")
save_cookies(driver)
driver.quit()
```

**Примечания:**
- Обновлять cookies раз в 5-7 дней
- Хранить копию cookies на случай порчи файла
- Добавить проверку валидности cookies перед парсингом

---

#### ОШИБКА #013: В результаты попадает реклама вместо органической выдачи

**Дата:** 22.01.2026
**Компонент:** yandex_parser.py
**Критичность:** Критическая

**Описание:**
При парсинге результатов Яндекса собираются ВСЕ результаты включая рекламные блоки (Яндекс.Директ). Это искажает анализ конкурентов, так как рекламные страницы:
- Не оптимизированы под SEO
- Имеют другую структуру контента
- Не являются реальными конкурентами по органической выдаче

**Симптомы:**
- В ТОП-10 попадают страницы с параметрами `?utm_source=direct`
- Объем таких страниц значительно меньше (лендинги)
- В подзаголовках отсутствует глубокая структура

**Требование пользователя:**
> "Когда заходим на страницу Яндекса, вбивая ключевой запрос, там есть реклама, вот нам эти страницы в априори брать не нужно"

**Решение:**
✅ Использовать фильтрацию по специальным маркерам Яндекс.Директ

```python
# ИНСТРУМЕНТЫ/скрипты/parsers/yandex_parser.py

class YandexSelectors:
    """CSS селекторы для фильтрации"""

    # Маркеры рекламы Яндекс.Директ (2026)
    AD_MARKERS = [
        ".label_theme_direct",           # Метка "Реклама"
        ".serp-adv__item",               # Рекламный блок
        "[data-fast-name='direct']",     # React компонент Direct
        ".VanillaReact[data-fast-name='direct']",  # Новый формат
        ".serp-item_type_direct",        # Тип результата = реклама
        ".organic_type_direct"           # Органика с рекламой
    ]

    # Маркеры видео (тоже нужно фильтровать)
    VIDEO_MARKERS = [
        ".serp-item_type_video",
        ".video-thumb",
        "[data-fast-name='video']"
    ]

def get_top_urls(driver, keyword: str, count: int = 10) -> List[str]:
    """Получить ТОП-N органических URL (БЕЗ рекламы)"""

    # ... код загрузки страницы ...

    results = driver.find_elements(By.CSS_SELECTOR, ".serp-item")
    urls = []

    for result in results:
        try:
            # КРИТИЧНО: Пропуск рекламы
            is_ad = False
            for ad_marker in YandexSelectors.AD_MARKERS:
                if result.find_elements(By.CSS_SELECTOR, ad_marker):
                    is_ad = True
                    logger.debug(f"Пропуск рекламы (маркер: {ad_marker})")
                    break

            if is_ad:
                continue  # Пропускаем рекламный результат

            # Пропуск видео
            is_video = False
            for video_marker in YandexSelectors.VIDEO_MARKERS:
                if result.find_elements(By.CSS_SELECTOR, video_marker):
                    is_video = True
                    logger.debug("Пропуск видео")
                    break

            if is_video:
                continue

            # Извлечение URL (только органика!)
            link = result.find_element(By.CSS_SELECTOR, "a.link")
            url = link.get_attribute("href")

            if url and url.startswith("http"):
                urls.append(url)
                logger.info(f"[Органика {len(urls)}] {url}")

            if len(urls) >= count:
                break

        except Exception as e:
            logger.warning(f"Ошибка обработки результата: {e}")
            continue

    return urls
```

**Альтернативное решение (через URL параметры):**
```python
# Дополнительная проверка по URL
def is_organic_url(url: str) -> bool:
    """Проверка что URL - органический результат"""
    # Рекламные URL часто содержат:
    ad_params = ['utm_source=direct', 'yclid=', 'from=direct']

    for param in ad_params:
        if param in url:
            logger.debug(f"URL содержит рекламный параметр: {param}")
            return False

    return True

# Использование
if url and url.startswith("http") and is_organic_url(url):
    urls.append(url)
```

**Примечания:**
- Обновлять список маркеров рекламы раз в квартал (Яндекс меняет классы)
- Логировать все пропущенные результаты для анализа
- При необходимости добавлять новые маркеры в `AD_MARKERS`
- Рекомендуется собирать 15-20 результатов, фильтровать, оставлять топ-10 органики

**Тестирование:**
```python
# Тест фильтрации
urls = get_top_urls(driver, "почему не худею", count=10)

# Проверить вручную первые 3 URL:
# 1. Открыть в браузере
# 2. Убедиться что это НЕ рекламная страница
# 3. Проверить что URL не содержит utm_source=direct
```

---

### КАТЕГОРИЯ 2: ПАРСИНГ КОНТЕНТА

#### ОШИБКА #004: H1 заголовок не найден на странице

**Дата:** Прогнозируемая ошибка
**Компонент:** content_parser.py
**Критичность:** Средняя

**Описание:**
Некоторые сайты используют нестандартные теги вместо `<h1>` для заголовка (например, `<div class="title">`).

**Симптомы:**
- `soup.find('h1')` возвращает `None`
- В результате заголовок = "Заголовок не найден"

**Текст ошибки:**
```python
AttributeError: 'NoneType' object has no attribute 'get_text'
```

**Решение:**
✅ Использовать множественные селекторы с fallback

```python
def extract_title(soup):
    """Извлечь заголовок с fallback стратегией"""
    # Попытка 1: Стандартный H1
    h1 = soup.find('h1')
    if h1:
        return h1.get_text(strip=True)

    # Попытка 2: Распространенные классы
    title_selectors = [
        '.title',
        '.article-title',
        '.post-title',
        '.entry-title',
        '[itemprop="headline"]',
        '.article__title',
        '.content-title'
    ]
    for selector in title_selectors:
        element = soup.select_one(selector)
        if element:
            return element.get_text(strip=True)

    # Попытка 3: Тег <title> (последняя надежда)
    title = soup.find('title')
    if title:
        return title.get_text(strip=True).split('|')[0].strip()

    # Если совсем ничего не найдено
    return "Заголовок не найден"
```

**Примечания:**
- Логировать URL сайтов, где H1 не найден
- Вручную проверить эти сайты и добавить их селекторы
- Обновить список селекторов по мере необходимости

---

#### ОШИБКА #005: Сайт защищен Cloudflare

**Дата:** Прогнозируемая ошибка
**Компонент:** content_parser.py
**Критичность:** Высокая

**Описание:**
Некоторые сайты (например, VC.ru, Medium) используют Cloudflare защиту от ботов. При обращении через `requests` получаем статус 403 Forbidden или страницу "Проверка браузера".

**Симптомы:**
- `response.status_code == 403`
- В HTML содержится "Checking your browser"
- BeautifulSoup парсит страницу Cloudflare вместо контента

**Текст ошибки:**
```python
requests.exceptions.HTTPError: 403 Client Error: Forbidden
```

**Решение:**
✅ Использовать Selenium для сайтов с Cloudflare

```python
import requests
from fake_useragent import UserAgent

def parse_article(url: str):
    """Парсинг статьи с fallback на Selenium"""

    # Попытка 1: Обычный requests
    try:
        ua = UserAgent()
        headers = {
            'User-Agent': ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9',
            'Referer': 'https://yandex.ru/'
        }
        response = requests.get(url, headers=headers, timeout=10)

        # Если Cloudflare блокирует
        if response.status_code == 403 or 'cf-browser-verification' in response.text:
            raise Exception("Cloudflare detected")

        soup = BeautifulSoup(response.text, 'html.parser')

    except:
        # Попытка 2: Selenium (медленнее, но обходит Cloudflare)
        from utils.browser import init_driver

        driver = init_driver()
        driver.get(url)
        time.sleep(3)  # Ждем загрузки Cloudflare challenge
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        driver.quit()

    # Парсинг контента
    title = extract_title(soup)
    # ...
    return data
```

**Примечания:**
- Вести список сайтов с Cloudflare
- Для этих сайтов сразу использовать Selenium
- Альтернатива: использовать сервисы типа ScraperAPI (платно)

---

#### ОШИБКА #006: Извлекается мусорный контент (реклама, меню, футер)

**Дата:** Прогнозируемая ошибка
**Компонент:** content_parser.py
**Критичность:** Средняя

**Описание:**
При извлечении текста через `soup.get_text()` захватывается не только текст статьи, но и навигация, реклама, комментарии.

**Симптомы:**
- Объем статьи завышен (вместо 10000 символов получается 50000)
- В тексте встречаются фразы из меню ("Главная", "Контакты", "О нас")
- Подзаголовки содержат текст из sidebar

**Решение:**
✅ Удалить мусорные теги и классы перед извлечением текста

```python
def clean_html(soup):
    """Удалить мусорные элементы"""

    # 1. Удалить теги
    unwanted_tags = [
        'script', 'style', 'nav', 'footer', 'header',
        'aside', 'iframe', 'noscript', 'svg', 'form',
        'button', 'input', 'select'
    ]
    for tag in soup(unwanted_tags):
        tag.decompose()

    # 2. Удалить по классам/id
    unwanted_selectors = [
        '[class*="ad"]',
        '[class*="banner"]',
        '[class*="sidebar"]',
        '[class*="comment"]',
        '[class*="related"]',
        '[class*="social"]',
        '[id*="comments"]',
        '[id*="sidebar"]'
    ]
    for selector in unwanted_selectors:
        for element in soup.select(selector):
            element.decompose()

    return soup

# Использование
soup = BeautifulSoup(html, 'html.parser')
soup = clean_html(soup)
text = soup.get_text(separator=' ', strip=True)
```

**Примечания:**
- Проверять первые результаты вручную
- Добавлять новые паттерны в `unwanted_selectors` по мере обнаружения
- Для разных CMS могут быть разные классы мусора

---

### КАТЕГОРИЯ 3: ТЕХНИЧЕСКИЕ ПРОБЛЕМЫ

#### ОШИБКА #007: undetected-chromedriver не может найти Chrome

**Дата:** Прогнозируемая ошибка
**Компонент:** utils/browser.py
**Критичность:** Критическая

**Описание:**
При инициализации undetected-chromedriver на macOS возникает ошибка, что Chrome не найден.

**Текст ошибки:**
```python
selenium.common.exceptions.SessionNotCreatedException:
Message: session not created: Chrome failed to start
```

**Решение:**
✅ Явно указать путь к Chrome

```python
import undetected_chromedriver as uc

options = uc.ChromeOptions()
# Явный путь к Chrome на macOS
options.binary_location = "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"

driver = uc.Chrome(options=options)
```

**Примечания:**
- Проверить, что Chrome установлен
- Проверить путь через `which google-chrome-stable` (Linux)
- На Windows путь обычно: `C:\Program Files\Google\Chrome\Application\chrome.exe`

---

#### ОШИБКА #008: SSL Certificate Error при парсинге HTTPS сайтов

**Дата:** Прогнозируемая ошибка
**Компонент:** content_parser.py
**Критичность:** Низкая

**Описание:**
При парсинге некоторых HTTPS сайтов через `requests` возникает ошибка проверки SSL сертификата.

**Текст ошибки:**
```python
requests.exceptions.SSLError:
HTTPSConnectionPool(host='example.com', port=443):
Max retries exceeded with url: /article (Caused by SSLError(...))
```

**Решение:**
✅ Отключить проверку SSL (не рекомендуется, но иногда необходимо)

```python
import requests
import urllib3

# Отключить предупреждения
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Запрос без проверки SSL
response = requests.get(url, verify=False, timeout=10)
```

**Безопасное решение:**
```python
import certifi

# Использовать актуальные сертификаты
response = requests.get(url, verify=certifi.where(), timeout=10)
```

**Примечания:**
- Использовать `verify=False` только для известных сайтов
- Обновить certifi: `pip install --upgrade certifi`

---

#### ОШИБКА #009: Timeout при загрузке медленного сайта

**Дата:** Прогнозируемая ошибка
**Компонент:** content_parser.py
**Критичность:** Средняя

**Описание:**
Некоторые сайты загружаются очень медленно (>10 сек), и requests выдает timeout.

**Текст ошибки:**
```python
requests.exceptions.Timeout:
HTTPConnectionPool(host='slowsite.com', port=80):
Read timed out. (read timeout=10)
```

**Решение:**
✅ Увеличить timeout и добавить retry логику

```python
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

def create_session_with_retries():
    """Сессия с автоматическими повторами"""
    session = requests.Session()

    retry = Retry(
        total=3,
        backoff_factor=1,  # 1, 2, 4 секунды задержки
        status_forcelist=[429, 500, 502, 503, 504],
    )

    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)

    return session

# Использование
session = create_session_with_retries()
response = session.get(url, timeout=30)  # 30 сек timeout
```

**Примечания:**
- Для критически важных сайтов увеличить timeout до 60 сек
- Логировать URL медленных сайтов
- Рассмотреть использование Selenium для таких сайтов

---

### КАТЕГОРИЯ 4: ЛОГИКА И ДАННЫЕ

#### ОШИБКА #010: Общие подзаголовки не находятся (все разные)

**Дата:** Прогнозируемая ошибка
**Компонент:** simple_analyzer.py
**Критичность:** Низкая

**Описание:**
При анализе конкурентов не находятся общие подзаголовки, потому что у всех 5 статей они написаны по-разному (синонимы, разная формулировка).

**Пример:**
```
Статья 1: "Проблемы с щитовидной железой"
Статья 2: "Заболевания щитовидки"
Статья 3: "Щитовидная железа и вес"
```

**Решение:**
✅ Использовать семантический анализ (в будущем через AI)

Временное решение (MVP):
```python
from difflib import SequenceMatcher

def similarity(a, b):
    """Процент схожести двух строк"""
    return SequenceMatcher(None, a.lower(), b.lower()).ratio()

def find_similar_headings(all_headings, threshold=0.7):
    """Найти похожие подзаголовки"""
    clusters = []

    for heading in all_headings:
        # Найти кластер, к которому подходит heading
        added = False
        for cluster in clusters:
            if similarity(heading, cluster[0]) >= threshold:
                cluster.append(heading)
                added = True
                break

        if not added:
            clusters.append([heading])

    # Вернуть кластеры с 3+ элементами
    common = [cluster[0] for cluster in clusters if len(cluster) >= 3]
    return common
```

**Примечания:**
- Для MVP использовать простое сравнение строк
- В Фазе 2 добавить AI анализ через Claude API
- Claude сможет определить, что "щитовидная железа" и "щитовидка" - это одно и то же

---

### КАТЕГОРИЯ 5: ОКРУЖЕНИЕ И ЗАВИСИМОСТИ

#### ОШИБКА #014: UnicodeDecodeError при активации venv с русским путем

**Дата:** 22.01.2026
**Компонент:** venv
**Критичность:** Критическая

**Описание:**
При попытке активировать виртуальное окружение venv, созданное в папке с русскими символами в пути, Python 3.9.6 на macOS выдает UnicodeDecodeError.

**Текст ошибки:**
```
Fatal Python error: init_import_site: Failed to import the site module
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xb0 in position 37: invalid start byte
```

**Контекст:**
Путь к проекту: `/Volumes/T7 Shield/SEO для инфобизнеса актуальность, возможности, инструменты/SEO_РАБОТА`

**Попытки решения:**
1. ❌ Активация через `source venv/bin/activate` - ошибка
2. ✅ Использование прямого пути к pip внутри venv

**Решение:**
Вместо активации venv, использовать прямой вызов pip:

```bash
# НЕ РАБОТАЕТ:
source venv/bin/activate
pip install -r requirements.txt

# РАБОТАЕТ:
/путь/к/проекту/venv/bin/pip install -r requirements.txt
```

Для macOS с русским путем:
```bash
cd "/Volumes/T7 Shield/SEO для инфобизнеса актуальность, возможности, инструменты/SEO_РАБОТА"
venv/bin/pip install --upgrade pip
venv/bin/pip install -r requirements.txt
```

Для запуска скриптов:
```bash
# Вместо python script.py после активации venv
venv/bin/python script.py
```

**Альтернативное решение (если не помогает):**
Создать симлинк на папку без русских символов:
```bash
# Создать симлинк
ln -s "/Volumes/T7 Shield/SEO для инфобизнеса актуальность, возможности, инструменты/SEO_РАБОТА" ~/seo_work

# Работать через симлинк
cd ~/seo_work
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

**Примечания:**
- Проблема специфична для Python 3.9 на macOS с русскими путями
- Python 3.11+ обрабатывает это лучше
- Рекомендуется использовать английские пути для проектов Python

---

#### ОШИБКА #011: ImportError при импорте undetected_chromedriver

**Дата:** Прогнозируемая ошибка
**Компонент:** utils/browser.py
**Критичность:** Критическая

**Описание:**
При запуске скрипта Python не может найти модуль `undetected_chromedriver`.

**Текст ошибки:**
```python
ModuleNotFoundError: No module named 'undetected_chromedriver'
```

**Решение:**
✅ Проверить активацию виртуального окружения и переустановить

```bash
# 1. Проверить активацию venv
which python3  # Должен показать путь внутри venv/

# 2. Если venv не активен
source venv/bin/activate  # macOS/Linux

# 3. Переустановить зависимости
pip install --upgrade pip
pip install -r requirements.txt

# 4. Проверить установку
python -c "import undetected_chromedriver; print('OK')"
```

**Примечания:**
- Всегда активировать venv перед запуском
- Добавить проверку активации в начало скрипта
- Использовать `python3 -m pip` вместо просто `pip` для точности

---

#### ОШИБКА #012: Конфликт версий Chrome и ChromeDriver

**Дата:** Прогнозируемая ошибка
**Компонент:** utils/browser.py
**Критичность:** Высокая

**Описание:**
undetected-chromedriver не может найти совместимый ChromeDriver для установленной версии Chrome.

**Текст ошибки:**
```python
selenium.common.exceptions.SessionNotCreatedException:
Message: session not created: This version of ChromeDriver only supports Chrome version 120
```

**Решение:**
✅ Указать версию вручную или обновить Chrome

```python
import undetected_chromedriver as uc

# Автоопределение версии Chrome
driver = uc.Chrome(use_subprocess=True)

# ИЛИ указать вручную
driver = uc.Chrome(version_main=120)  # Версия Chrome
```

**Если не помогает:**
```bash
# Обновить Chrome до последней версии
# macOS: Открыть Chrome → Меню → О Google Chrome → Обновить

# Очистить кеш undetected-chromedriver
rm -rf ~/Library/Application\ Support/undetected_chromedriver/

# Переустановить библиотеку
pip uninstall undetected-chromedriver
pip install undetected-chromedriver --upgrade
```

**Примечания:**
- Регулярно обновлять Chrome
- undetected-chromedriver автоматически скачивает нужный драйвер
- В крайнем случае использовать `webdriver-manager`

---

## ШАБЛОН ДЛЯ НОВЫХ ОШИБОК

```markdown
### ОШИБКА #0XX: Краткое описание

**Дата:** ДД.ММ.ГГГГ
**Компонент:** file.py
**Критичность:** Высокая/Средняя/Низкая

**Описание:**
Что произошло

**Симптомы:**
- Симптом 1
- Симптом 2

**Текст ошибки:**
\`\`\`
Полный traceback
\`\`\`

**Контекст:**
Что делалось перед ошибкой

**Попытки решения:**
1. Попытка 1 - результат
2. Попытка 2 - результат

**Решение:**
✅ Что сработало

\`\`\`python
# Код решения
\`\`\`

**Примечания:**
Дополнительная информация
```

---

## СТАТИСТИКА ОШИБОК

**Всего задокументировано:** 16 ошибок
**Критических:** 6
**Высоких:** 4
**Средних:** 5
**Низких:** 1

**Категории:**
- Яндекс антибот: 4 ошибки
- Парсинг контента: 4 ошибки
- Технические проблемы: 3 ошибки
- Логика и данные: 1 ошибка
- Окружение: 2 ошибки
- Проблемы с библиотеками: 2 ошибки (Chrome 144 + несовместимость)

---

**ПОСЛЕДНЕЕ ОБНОВЛЕНИЕ:** 22.01.2026 (добавлена ОШИБКА #016: нестандартные теги подзаголовков)

**СЛЕДУЮЩИЙ ПЕРЕСМОТР:** После анализа результатов на 10+ запросах

---

### КАТЕГОРИЯ 6: ПРОБЛЕМЫ С БИБЛИОТЕКАМИ

#### ОШИБКА #014: undetected-chromedriver несовместим с Chrome 144

**Дата:** 22.01.2026
**Компонент:** utils/browser.py
**Критичность:** Критическая

**Описание:**
undetected-chromedriver версии 3.5.5 (последняя в PyPI) не поддерживает Chrome 144. При запуске драйвера Chrome открывается и сразу закрывается с ошибкой "target window already closed".

**Симптомы:**
- Chrome запускается
- Окно моментально закрывается
- Ошибка: `NoSuchWindowException: target window already closed`
- Session info показывает: `chrome=144.0.7559.96`

**Текст ошибки:**
```
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=144.0.7559.96)
```

**Контекст:**
- Установлен Chrome 144 (последняя версия на 22.01.2026)
- undetected-chromedriver 3.5.5 из PyPI
- Даже версия с GitHub не решила проблему

**Попытки решения:**
1. ❌ Обновление до версии с GitHub - не помогло
2. ❌ Обновление Selenium до 4.36.0 - не помогло
3. ❌ Использование selenium-stealth - вызвало дополнительные ошибки
4. ❌ Отключение профиля Chrome - не помогло

**Решение:**
✅ Использовать обычный Selenium (без undetected-chromedriver)

```python
# ПРОСТОЕ РЕШЕНИЕ
from selenium import webdriver

options = webdriver.ChromeOptions()
driver = webdriver.Chrome(options=options)
```

**Альтернативы:**
1. Понизить Chrome до версии 120-130 (не рекомендуется)
2. Использовать Playwright вместо Selenium (будущее решение)
3. Работать с обычным Selenium + cookies для обхода CAPTCHA

**Примечания:**
- undetected-chromedriver полезен для обхода антибота, НО не обязателен
- Для MVP достаточно обычного Selenium
- Если Яндекс покажет CAPTCHA - сохранить cookies после ручного решения

---

#### ОШИБКА #015: selenium-stealth конфликтует с Chrome 144

**Дата:** 22.01.2026
**Компонент:** utils/browser.py
**Критичность:** Высокая

**Описание:**
При попытке применить stealth() к драйверу Chrome 144 возникает ошибка "target window already closed". Библиотека пытается выполнить CDP команды на уже закрытом окне.

**Текст ошибки:**
```python
File "/Users/ivan/Library/Python/3.9/lib/python/site-packages/selenium_stealth/__init__.py", line 46, in stealth
    chrome_app(driver, **kwargs)
...
selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed
```

**Решение:**
✅ Не использовать selenium-stealth

**Причина:**
- selenium-stealth не обновлялся с 2021 года
- Не совместим с новыми версиями Chrome и Selenium

**Примечания:**
- undetected-chromedriver сам делает маскировку, stealth не нужен
- Для простого Selenium маскировка не критична на старте

---

#### ОШИБКА #016: Сайты не используют стандартные H2/H3 теги

**Дата:** 22.01.2026
**Компонент:** content_parser.py
**Критичность:** Средняя

**Описание:**
При тестировании на реальных медицинских сайтах (ТОП Яндекса по запросу "почему не худею") обнаружено, что большинство сайтов НЕ используют стандартные теги `<h2>` и `<h3>` для подзаголовков. Вместо этого используются:
- `<div class="subtitle">`
- `<p><strong>Подзаголовок</strong></p>`
- `<span class="heading">`

**Тест-кейс:**
Запрос: "почему не худею", ТОП-5 сайтов

**Результаты:**
```
[1] 52gkb.ru - H2: 0, H3: 0 (6020 символов есть!)
[2] clinica-repromed.ru - H2: 0, H3: 3
[3] istclinic.ru - H2: 1, H3: 7
[4] unclinic.ru - H2: 0, H3: 0 (0 символов - не спарсилось)
```

**Симптомы:**
- Контент есть (6000+ символов)
- H1 извлекается корректно
- H2/H3 = 0 у большинства

**Решение (для MVP):**
✅ Оставить как есть - для MVP достаточно

**Будущее улучшение (Фаза 2):**
Использовать AI для извлечения семантических подзаголовков:

```python
def extract_semantic_headings(soup, text: str) -> List[str]:
    """Извлечь подзаголовки через Claude API"""

    prompt = f"""
    Проанализируй текст статьи и извлеки ВСЕ подзаголовки/темы.

    Текст: {text[:5000]}

    Верни список подзаголовков (один на строку).
    """

    # Вызов Claude API
    response = anthropic.messages.create(
        model="claude-3-haiku-20240307",
        messages=[{"role": "user", "content": prompt}]
    )

    headings = response.content[0].text.strip().split('\n')
    return headings
```

**Примечания:**
- Проблема специфична для медицинских сайтов на CMS
- Для блогов (WordPress) проблема встречается реже
- AI-решение планируется в Фазе 2 (интеграция Claude API)

---

